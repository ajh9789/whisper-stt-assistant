from datetime import datetime
import numpy as np
import sounddevice as sd
import torch
import torchaudio
from faster_whisper import WhisperModel
from multiprocessing import Process, Queue
import time

# =============================
# ğŸ¯ ì„¤ì • ê°’
# =============================

MODEL_SIZE = "large-v3"
DEVICE_ID = 1
RECORD_SECONDS = 10
CHANNELS = 1
ENERGY_GATE_THRESHOLD = 0.0007

# =============================
# ğŸ§ ë””ë°”ì´ìŠ¤ ë° ëª¨ë¸ ì´ˆê¸°í™”
# =============================

device_info = sd.query_devices(DEVICE_ID, 'input')
SAMPLE_RATE = int(device_info['default_samplerate'])
blocksize = int(SAMPLE_RATE * 2)  # nì´ˆ ë‹¨ìœ„ ë¸”ë¡

# =============================
# ğŸ” ì˜¤ë””ì˜¤ ìº¡ì²˜ ì½œë°±
# =============================

def audio_callback(indata, frames, time_info, status, queue):
    if status:
        print("âš ï¸", status)
    queue.put(indata.copy())

def start_recording(queue: Queue):
    def callback(indata, frames, time_info, status):
        audio_callback(indata, frames, time_info, status, queue)

    with sd.InputStream(
        device=DEVICE_ID,
        channels=CHANNELS,
        samplerate=SAMPLE_RATE,
        blocksize=blocksize,
        dtype='float32',
        callback=callback
    ):
        while True:
            time.sleep(0.1)

# =============================
# ğŸ§  STT ì²˜ë¦¬ í”„ë¡œì„¸ìŠ¤
# =============================

def process_audio(queue: Queue):
    model = WhisperModel(MODEL_SIZE, device="cuda" if torch.cuda.is_available() else "cpu", compute_type="float16")
    print(f"ğŸš€ STT ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    audio_chunks = []
    total_frames = 0
    target_frames = SAMPLE_RATE * RECORD_SECONDS
    last_spoken_time = time.time()
    silence_interval = 5  # ë¬´ìŒ ê²½ê³  ê¸°ì¤€ ì´ˆ

    while True:
        chunk = queue.get()
        audio_chunks.append(chunk)
        total_frames += chunk.shape[0]

        if total_frames >= target_frames:
            audio = np.concatenate(audio_chunks, axis=0)
            audio_chunks.clear()
            total_frames = 0

            audio_tensor = torch.from_numpy(audio).squeeze()

            # ë¬´ìŒ í•„í„°
            if torch.mean(torch.abs(audio_tensor)) < ENERGY_GATE_THRESHOLD:
                # ë¬´ìŒ ì‹œê°„ ì²´í¬
                if time.time() - last_spoken_time >= silence_interval:
                    print(f"ğŸ”‡ [ë¬´ìŒ ìƒíƒœ: {silence_interval}ì´ˆ ì´ìƒ]")
                    last_spoken_time = time.time()
                continue

            # ë¦¬ìƒ˜í”Œë§
            if SAMPLE_RATE != 16000:
                audio_tensor = torchaudio.functional.resample(audio_tensor, orig_freq=SAMPLE_RATE, new_freq=16000).contiguous()

            audio_array = audio_tensor.cpu().numpy()

            # STT ì‹¤í–‰
            segments, _ = model.transcribe(
                audio_array,
                language="ko",
                vad_filter=True,
                beam_size=1,
                temperature=0.0
            )

            for segment in segments:
                print(f"{segment.text.strip()}")
                last_spoken_time = time.time()  # ë°œí™” ì‹œê°„ ê°±ì‹ 

# =============================
# ğŸ¯ ì‹¤í–‰
# =============================

if __name__ == "__main__":
    print(f"\nğŸ™ï¸ ë””ë°”ì´ìŠ¤ {DEVICE_ID}: {device_info['name']} ({SAMPLE_RATE} Hz)")
    print(f"âœ… FasterWhisper {MODEL_SIZE} ëª¨ë¸ ë¡œë“œ ì˜ˆì •")
    print(f"ğŸ”„ {RECORD_SECONDS}ì´ˆë§ˆë‹¤ ìë™ ë…¹ìŒ + ë³€í™˜ ì‹œì‘ (Ctrl+Cë¡œ ì¤‘ì§€)\n")

    queue = Queue(maxsize=10)

    recorder = Process(target=start_recording, args=(queue,))
    stt_worker = Process(target=process_audio, args=(queue,))

    recorder.start()
    stt_worker.start()

    try:
        recorder.join()
        stt_worker.join()
    except KeyboardInterrupt:
        recorder.terminate()
        stt_worker.terminate()
        print("\nğŸ›‘ í”„ë¡œê·¸ë¨ ì¢…ë£Œë¨.")

