from datetime import datetime
import numpy as np
import sounddevice as sd
import torch
import torchaudio
from faster_whisper import WhisperModel
from multiprocessing import Process, Queue
import time

# =============================
# üéØ ÏÑ§Ï†ï Í∞í
# =============================

MODEL_SIZE = "large-v3"
DEVICE_ID = 1
RECORD_SECONDS = 5
CHANNELS = 1
ENERGY_GATE_THRESHOLD = 0.0008

# =============================
# üéß ÎîîÎ∞îÏù¥Ïä§ Î∞è Î™®Îç∏ Ï¥àÍ∏∞Ìôî
# =============================

device_info = sd.query_devices(DEVICE_ID, 'input')
SAMPLE_RATE = int(device_info['default_samplerate'])
blocksize = int(SAMPLE_RATE * 2.5)  # nÏ¥à Îã®ÏúÑ Î∏îÎ°ù

# =============================
# üîÅ Ïò§ÎîîÏò§ Ï∫°Ï≤ò ÏΩúÎ∞±
# =============================

def audio_callback(indata, frames, time_info, status, queue):
    if status:
        print("‚ö†Ô∏è", status)
    queue.put(indata.copy())

def start_recording(queue: Queue):
    def callback(indata, frames, time_info, status):
        audio_callback(indata, frames, time_info, status, queue)

    with sd.InputStream(
        device=DEVICE_ID,
        channels=CHANNELS,
        samplerate=SAMPLE_RATE,
        blocksize=blocksize,
        dtype='float32',
        callback=callback
    ):
        while True:
            time.sleep(0.1)

# =============================
# üß† STT Ï≤òÎ¶¨ ÌîÑÎ°úÏÑ∏Ïä§
# =============================

def process_audio(queue: Queue):
    model = WhisperModel(MODEL_SIZE, device="cuda" if torch.cuda.is_available() else "cpu", compute_type="float16")
    print(f"üöÄ STT Î™®Îç∏ Î°úÎìú ÏôÑÎ£å")

    audio_chunks = []
    total_frames = 0
    target_frames = SAMPLE_RATE * RECORD_SECONDS

    while True:
        chunk = queue.get()
        audio_chunks.append(chunk)
        total_frames += chunk.shape[0]

        if total_frames >= target_frames:
            audio = np.concatenate(audio_chunks, axis=0)
            audio_chunks.clear()
            total_frames = 0

            audio_tensor = torch.from_numpy(audio).squeeze()

            # Î¨¥Ïùå ÌïÑÌÑ∞
            if torch.mean(torch.abs(audio_tensor)) < ENERGY_GATE_THRESHOLD:
                continue

            # Î¶¨ÏÉòÌîåÎßÅ
            if SAMPLE_RATE != 16000:
                audio_tensor = torchaudio.functional.resample(audio_tensor, orig_freq=SAMPLE_RATE, new_freq=16000)

            audio_array = audio_tensor.cpu().numpy()

            # STT Ïã§Ìñâ
            segments, _ = model.transcribe(
                audio_array,
                language="ko",
                vad_filter=True,
                beam_size=1,
                temperature=0.0
            )
            #[{datetime.now().strftime('%H:%M:%S')}]:
            for segment in segments:
                print(f"{segment.text.strip()}")

# =============================
# üéØ Ïã§Ìñâ
# =============================

if __name__ == "__main__":
    print(f"\nüéôÔ∏è ÎîîÎ∞îÏù¥Ïä§ {DEVICE_ID}: {device_info['name']} ({SAMPLE_RATE} Hz)")
    print(f"‚úÖ FasterWhisper {MODEL_SIZE} Î™®Îç∏ Î°úÎìú ÏòàÏ†ï")
    print(f"üîÑ {RECORD_SECONDS}Ï¥àÎßàÎã§ ÏûêÎèô ÎÖπÏùå + Î≥ÄÌôò ÏãúÏûë (Ctrl+CÎ°ú Ï§ëÏßÄ)\n")

    queue = Queue(maxsize=10)

    recorder = Process(target=start_recording, args=(queue,))
    stt_worker = Process(target=process_audio, args=(queue,))

    recorder.start()
    stt_worker.start()

    try:
        recorder.join()
        stt_worker.join()
    except KeyboardInterrupt:
        recorder.terminate()
        stt_worker.terminate()
        print("\nüõë ÌîÑÎ°úÍ∑∏Îû® Ï¢ÖÎ£åÎê®.")

